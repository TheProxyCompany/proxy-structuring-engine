{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install required packages\n",
    "!uv pip install -U pse # proxy structuring engine\n",
    "!uv pip install sentencepiece\n",
    "!uv pip install accelerate\n",
    "!uv pip install transformers\n",
    "!uv pip install torch\n",
    "!uv pip install numpy\n",
    "!uv pip install bitsandbytes\n",
    "!uv pip install sentencepiece\n",
    "!uv pip install protobuf\n",
    "!uv pip install -U tqdm\n",
    "!uv pip install ipywidgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from pse.engine.structuring_engine import StructuringEngine\n",
    "from pse.util.torch_mixin import PSETorchMixin\n",
    "\n",
    "# toggle this to logging.DEBUG to see the PSE debug logs!\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "class PSE_Torch(PSETorchMixin, LlamaForCausalLM):\n",
    "    pass\n",
    "\n",
    "# you can change the model path to any other model on huggingface\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = PSE_Torch.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id[0]\n",
    "if model.generation_config:\n",
    "    model.generation_config.top_p = None\n",
    "    model.generation_config.top_k = 8\n",
    "    model.generation_config.do_sample = True\n",
    "    model.generation_config.temperature = 1.0\n",
    "    model.generation_config.max_new_tokens = 1000\n",
    "    model.generation_config.pad_token_id = model.config.eos_token_id[0]\n",
    "\n",
    "# create structuring engine normally\n",
    "model.engine = StructuringEngine(tokenizer, multi_token_sampling=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title define custom state machines\n",
    "# create a state machine that combines the thinking and answer state machines\n",
    "from pse_core.state_machine import StateMachine\n",
    "\n",
    "from pse.types.base.character import CharacterStateMachine\n",
    "from pse.types.base.encapsulated import EncapsulatedStateMachine\n",
    "\n",
    "thinking_delimiters = (\"[thinking]\", \"[/thinking]\")\n",
    "answer_delimiters = (\"[answer]\", \"[/answer]\")\n",
    "\n",
    "# encapsulated state machines are used to allow a language model\n",
    "# to generate unstructured content before the structured output\n",
    "# starts. This \"scratchpad\" is disabled by default (min_buffer_length=-1)\n",
    "thinking_state_machine = EncapsulatedStateMachine(\n",
    "    state_machine=CharacterStateMachine(\n",
    "        charset=\"\",  # empty charset means any character is valid\n",
    "        blacklist_charset=\"[\",  # the character that starts the delimiter is blacklisted,\n",
    "        char_min=100,  # no minimum number of characters\n",
    "        char_limit=500,  # 500 characters is the maximum\n",
    "    ),\n",
    "    delimiters=thinking_delimiters,\n",
    ")\n",
    "# the answer state machine is used to wrap the structured output\n",
    "answer_state_machine = EncapsulatedStateMachine(\n",
    "    state_machine=CharacterStateMachine(\n",
    "        charset=\"\",  # empty charset means any character is valid\n",
    "        blacklist_charset=\"[\",  # the character that starts the delimiter is blacklisted,\n",
    "        char_min=None,  # no minimum number of characters\n",
    "        char_limit=None,  # no maximum number of characters\n",
    "    ),\n",
    "    delimiters=answer_delimiters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output:\n",
      "\u001b[33m[thinking]Okay, let me consider this question. Generation 1 (Gen 1) Pokémon... that's an interesting era. I've got some fond memories of playing Pokémon Gold, Silver, and Crystal with my friends. \n",
      "\n",
      "[/thinking][thinking]You know, I've always been a fan of the classic starters. Which one, you ask? Well, I think my favorite is... Bulbasaur! \n",
      "\n",
      "[/thinking][thinking]Why Bulbasaur, you ask? It's just so versatile, you know? Its ability to learn both Poison and Grass types makes it a force to be reckoned with in the early game. And let's be honest, its evolutions are just as cool - Venusaur and Ivysaur, the two of them. \n",
      "\n",
      "[/thinking][answer]I love playing Pokémon, and I'm a big fan of the classic starters. Bulbasaur is definitely one of my favorites. Have you ever played Pokémon or want to start a new adventure? I'd be happy to chat with you about it! [/answer]\u001b[0m\n",
      "\n",
      "\u001b[3mOkay, let me consider this question. Generation 1 (Gen 1) Pokémon... that's an interesting era. I've got some fond memories of playing Pokémon Gold, Silver, and Crystal with my friends. \n",
      "\n",
      "\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I love playing Pokémon, and I'm a big fan of the classic starters. Bulbasaur is definitely one of my favorites. Have you ever played Pokémon or want to start a new adventure? I'd be happy to chat with you about it! \n"
     ]
    }
   ],
   "source": [
    "# Configure the engine with a state machine that enforces the following flow:\n",
    "#\n",
    "# The model starts in the 'thinking' state where it can express its reasoning.\n",
    "# From there, it enters a 'verify' state where it can either:\n",
    "# 1. Think more by returning to the thinking state\n",
    "# 2. Provide its final answer by transitioning to the answer state\n",
    "#\n",
    "#      ┌──────────────────────┐\n",
    "#      │                      │\n",
    "#      ▼                      │\n",
    "# ┌──────────┐          ┌──────────┐          ┌──────────┐\n",
    "# │          │          │          │          │          │\n",
    "# │ thinking ├─────────►│  verify  ├─────────►│  answer  │\n",
    "# │          │          │          │          │          │\n",
    "# └──────────┘          └──────────┘          └──────────┘\n",
    "#\n",
    "# This ensures the model follows a structured thought process before\n",
    "# providing its final answer.\n",
    "#\n",
    "from pse.types.base.loop import LoopStateMachine\n",
    "\n",
    "model.engine.configure(\n",
    "    StateMachine(\n",
    "        {\n",
    "            \"thinking\": [\n",
    "                (\n",
    "                    LoopStateMachine(\n",
    "                        thinking_state_machine,\n",
    "                        min_loop_count=1,\n",
    "                        max_loop_count=2,\n",
    "                    ),\n",
    "                    \"answer\",\n",
    "                )\n",
    "            ],\n",
    "            \"answer\": [\n",
    "                (\n",
    "                    answer_state_machine,\n",
    "                    \"done\",\n",
    "                ),\n",
    "            ],\n",
    "        },\n",
    "        start_state=\"thinking\",\n",
    "        end_states=[\"done\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    f\"Reason step by step using delimiters to seperate your thought process.\\n\"\n",
    "    \"For example, when asked a question, you should think and then answer.\\n\"\n",
    "    \"Example:\\n\"\n",
    "    f\"{thinking_delimiters[0]}your step by step thinking here{thinking_delimiters[1]}\"\n",
    "    f\"{answer_delimiters[0]}your answer here{answer_delimiters[1]}\\n\"\n",
    "    \"you can think multiple times before providing your answer.\\n\\n\"\n",
    ")\n",
    "prompt = \"You just walked into Professor Oak's Lab - please pick a Kanto starter pokemon.\"\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "assert isinstance(input_ids, torch.Tensor)\n",
    "input_ids = input_ids.to(model.device)\n",
    "assert isinstance(input_ids, torch.Tensor)\n",
    "output = model.generate(input_ids)\n",
    "\n",
    "structured_output = model.engine.parse_structured_output()\n",
    "print(f\"raw output:\\n\\x1b[33m{structured_output}\\x1b[0m\\n\")\n",
    "print(\n",
    "    f\"\\x1b[3m{structured_output.split(thinking_delimiters[0])[1].split(thinking_delimiters[1])[0]}\\x1b[0m\"\n",
    ")\n",
    "print(100 * \"-\")\n",
    "print(structured_output.split(answer_delimiters[0])[1].split(answer_delimiters[1])[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
