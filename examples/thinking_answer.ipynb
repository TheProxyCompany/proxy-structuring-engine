{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install required packages\n",
    "!uv pip install -U pse # proxy structuring engine\n",
    "!uv pip install sentencepiece\n",
    "!uv pip install accelerate\n",
    "!uv pip install transformers\n",
    "!uv pip install torch\n",
    "!uv pip install numpy\n",
    "!uv pip install bitsandbytes\n",
    "!uv pip install sentencepiece\n",
    "!uv pip install protobuf\n",
    "!uv pip install -U tqdm\n",
    "!uv pip install ipywidgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from pse.engine.structuring_engine import StructuringEngine\n",
    "from pse.util.torch_mixin import PSETorchMixin\n",
    "\n",
    "# toggle this to logging.DEBUG to see the PSE debug logs!\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "class PSE_Torch(PSETorchMixin, LlamaForCausalLM):\n",
    "    pass\n",
    "\n",
    "# you can change the model path to any other model on huggingface\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = PSE_Torch.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id[0]\n",
    "if model.generation_config:\n",
    "    model.generation_config.top_p = None\n",
    "    model.generation_config.top_k = 8\n",
    "    model.generation_config.do_sample = True\n",
    "    model.generation_config.temperature = 1.0\n",
    "    model.generation_config.min_p = 0.02\n",
    "    model.generation_config.max_new_tokens = 1000\n",
    "    model.generation_config.generation_kwargs = {\"logits_to_keep\": 8}\n",
    "    model.generation_config.pad_token_id = model.config.eos_token_id[0]\n",
    "\n",
    "# create structuring engine normally\n",
    "model.engine = StructuringEngine(tokenizer, multi_token_sampling=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title define custom state machines\n",
    "# create a state machine that combines the thinking and answer state machines\n",
    "from pse_core.state_machine import StateMachine\n",
    "\n",
    "from pse.types.base.character import CharacterStateMachine\n",
    "from pse.types.base.encapsulated import EncapsulatedStateMachine\n",
    "\n",
    "thinking_delimiters = (\"[thinking]\", \"[/thinking]\")\n",
    "answer_delimiters = (\"[answer]\", \"[/answer]\")\n",
    "\n",
    "# encapsulated state machines are used to allow a language model\n",
    "# to generate unstructured content before the structured output\n",
    "# starts. This \"scratchpad\" is disabled by default (min_buffer_length=-1)\n",
    "thinking_state_machine = EncapsulatedStateMachine(\n",
    "    state_machine=CharacterStateMachine(\n",
    "        charset=\"\",  # empty charset means any character is valid\n",
    "        blacklist_charset=\"[\",  # the character that starts the delimiter is blacklisted,\n",
    "        char_min=100,  # no minimum number of characters\n",
    "        char_limit=500,  # 500 characters is the maximum\n",
    "    ),\n",
    "    delimiters=thinking_delimiters,\n",
    ")\n",
    "# the answer state machine is used to wrap the structured output\n",
    "answer_state_machine = EncapsulatedStateMachine(\n",
    "    state_machine=CharacterStateMachine(\n",
    "        charset=\"\",  # empty charset means any character is valid\n",
    "        blacklist_charset=\"[\",  # the character that starts the delimiter is blacklisted,\n",
    "        char_min=None,  # no minimum number of characters\n",
    "        char_limit=None,  # no maximum number of characters\n",
    "    ),\n",
    "    delimiters=answer_delimiters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the engine with a state machine that enforces the following flow:\n",
    "#\n",
    "# The model starts in the 'thinking' state where it can express its reasoning.\n",
    "# From there, it enters a 'verify' state where it can either:\n",
    "# 1. Think more by returning to the thinking state\n",
    "# 2. Provide its final answer by transitioning to the answer state\n",
    "#\n",
    "#      ┌──────────────────────┐\n",
    "#      │                      │\n",
    "#      ▼                      │\n",
    "# ┌──────────┐          ┌──────────┐          ┌──────────┐\n",
    "# │          │          │          │          │          │\n",
    "# │ thinking ├─────────►│  verify  ├─────────►│  answer  │\n",
    "# │          │          │          │          │          │\n",
    "# └──────────┘          └──────────┘          └──────────┘\n",
    "#\n",
    "# This ensures the model follows a structured thought process before\n",
    "# providing its final answer.\n",
    "#\n",
    "model.engine.configure(\n",
    "    StateMachine(\n",
    "        start_state=\"thinking\",\n",
    "        end_states=[\"answer\"],\n",
    "        state_graph={\n",
    "            \"thinking\": [\n",
    "                (\n",
    "                    thinking_state_machine,\n",
    "                    \"verify\",\n",
    "                )\n",
    "            ],\n",
    "            \"verify\": [\n",
    "                (\n",
    "                    thinking_state_machine,\n",
    "                    \"thinking\",\n",
    "                ),\n",
    "                (\n",
    "                    answer_state_machine,\n",
    "                    \"answer\",\n",
    "                ),\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    f\"Reason step by step using delimiters to seperate your thought process.\\n\"\n",
    "    \"For example, when asked a question, you should think and then answer.\\n\"\n",
    "    \"Example:\\n\"\n",
    "    f\"{thinking_delimiters[0]}your step by step thinking here{thinking_delimiters[1]}\"\n",
    "    f\"{answer_delimiters[0]}your answer here{answer_delimiters[1]}\\n\"\n",
    "    \"you can think multiple times before providing your answer.\\n\\n\"\n",
    ")\n",
    "prompt = (\n",
    "    \"What is the capital of Virginia, and what major US city is Arlington closest to?\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "assert isinstance(input_ids, torch.Tensor)\n",
    "input_ids = input_ids.to(model.device)\n",
    "assert isinstance(input_ids, torch.Tensor)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "structured_output = model.engine.parse_structured_output()\n",
    "print(f\"raw output:\\n\\x1b[33m{structured_output}\\x1b[0m\\n\")\n",
    "print(\n",
    "    f\"\\x1b[3m{structured_output.split(thinking_delimiters[0])[1].split(thinking_delimiters[1])[0]}\\x1b[0m\"\n",
    ")\n",
    "print(100 * \"-\")\n",
    "print(structured_output.split(answer_delimiters[0])[1].split(answer_delimiters[1])[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
