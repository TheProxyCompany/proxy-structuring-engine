{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install required packages\n",
    "!uv pip install -U pse # proxy structuring engine\n",
    "!uv pip install sentencepiece\n",
    "!uv pip install accelerate\n",
    "!uv pip install transformers\n",
    "!uv pip install torch\n",
    "!uv pip install numpy\n",
    "!uv pip install bitsandbytes\n",
    "!uv pip install sentencepiece\n",
    "!uv pip install protobuf\n",
    "!uv pip install -U tqdm\n",
    "!uv pip install ipywidgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "from pse.structuring_engine import StructuringEngine\n",
    "from pse.util.torch_mixin import PSETorchMixin\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # toggle this to logging.DEBUG to see the PSE debug logs!\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "class PSE_Torch(PSETorchMixin, LlamaForCausalLM):\n",
    "    pass\n",
    "\n",
    "# you can change the model path to any other model on huggingface\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = PSE_Torch.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id[0]\n",
    "if model.generation_config:\n",
    "    model.generation_config.top_p = None\n",
    "    model.generation_config.top_k = 8\n",
    "    model.generation_config.do_sample = True\n",
    "    model.generation_config.temperature = 1.0\n",
    "    model.generation_config.max_new_tokens = 1000\n",
    "    model.generation_config.pad_token_id = model.config.eos_token_id[0]\n",
    "\n",
    "# create structuring engine normally\n",
    "model.engine = StructuringEngine(tokenizer, multi_token_sampling=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title define custom state machines\n",
    "# create a state machine that combines the thinking and answer state machines\n",
    "from pse_core.state_machine import StateMachine\n",
    "\n",
    "from pse.types.base.character import CharacterStateMachine\n",
    "from pse.types.base.encapsulated import EncapsulatedStateMachine\n",
    "\n",
    "thinking_delimiters = (\"[thinking]\", \"[/thinking]\")\n",
    "answer_delimiters = (\"[answer]\", \"[/answer]\")\n",
    "\n",
    "# encapsulated state machines are used to allow a language model\n",
    "# to generate unstructured content before the structured output\n",
    "# starts. This \"scratchpad\" is disabled by default (min_buffer_length=-1)\n",
    "thinking_state_machine = EncapsulatedStateMachine(\n",
    "    state_machine=CharacterStateMachine(\n",
    "        charset=\"\",  # empty charset means any character is valid\n",
    "        blacklist_charset=\"[\",  # the character that starts the delimiter is blacklisted,\n",
    "        char_min=None,  # no minimum number of characters\n",
    "        char_limit=None,  # 500 characters is the maximum\n",
    "    ),\n",
    "    delimiters=thinking_delimiters,\n",
    ")\n",
    "# the answer state machine is used to wrap the structured output\n",
    "answer_state_machine = EncapsulatedStateMachine(\n",
    "    state_machine=CharacterStateMachine(\n",
    "        charset=\"\",  # empty charset means any character is valid\n",
    "        blacklist_charset=\"[\",  # the character that starts the delimiter is blacklisted,\n",
    "        char_min=None,  # no minimum number of characters\n",
    "        char_limit=None,  # no maximum number of characters\n",
    "    ),\n",
    "    delimiters=answer_delimiters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output:\n",
      "\u001b[33m[thinking]favorite color... I've been thinking about it for a while now... [/thinking][thinking]Hmm, I really like... [/thinking][answer]Blue, my favorite color! I think it's just so calming and relaxing. It's a color that always puts me in a good mood. Plus, it's just so versatile - I can pair it with just about anything! [/answer]\u001b[0m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Blue, my favorite color! I think it's just so calming and relaxing. It's a color that always puts me in a good mood. Plus, it's just so versatile - I can pair it with just about anything! \n"
     ]
    }
   ],
   "source": [
    "# Configure the engine with a state machine that enforces the following flow:\n",
    "#\n",
    "# The model starts in the 'thinking' state where it can express its reasoning.\n",
    "# From there, it can transition to providing its final answer.\n",
    "#\n",
    "#      ┌──────────────┐\n",
    "#      │              │\n",
    "#      ▼              │\n",
    "# ┌──────────┐        │\n",
    "# │          │        │\n",
    "# │ thinking ├────────┘\n",
    "# │          │\n",
    "# └──────┬───┘\n",
    "#        │\n",
    "#        ▼\n",
    "# ┌──────────┐\n",
    "# │          │\n",
    "# │  answer  │\n",
    "# │          │\n",
    "# └──────────┘\n",
    "#\n",
    "# This ensures the model follows a structured thought process before\n",
    "# providing its final answer.\n",
    "\n",
    "from pse.types.base.loop import LoopStateMachine\n",
    "\n",
    "model.engine.configure(\n",
    "    StateMachine(\n",
    "        {\n",
    "            \"thinking\": [\n",
    "                (\n",
    "                    LoopStateMachine(\n",
    "                        thinking_state_machine,\n",
    "                        min_loop_count=1,\n",
    "                        max_loop_count=2,\n",
    "                    ),\n",
    "                    \"answer\",\n",
    "                )\n",
    "            ],\n",
    "            \"answer\": [\n",
    "                (\n",
    "                    answer_state_machine,\n",
    "                    \"done\",\n",
    "                ),\n",
    "            ],\n",
    "        },\n",
    "        start_state=\"thinking\",\n",
    "        end_states=[\"done\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    f\"Reason step by step using delimiters to seperate your thought process.\\n\"\n",
    "    \"For example, when asked a question, you should think and then answer.\\n\"\n",
    "    \"Example:\\n\"\n",
    "    f\"{thinking_delimiters[0]}your step by step thinking here{thinking_delimiters[1]}\"\n",
    "    f\"{answer_delimiters[0]}your answer here{answer_delimiters[1]}\\n\"\n",
    "    \"you can think multiple times before providing your answer.\\n\\n\"\n",
    ")\n",
    "prompt = \"Please pick a favorite color. Think about it first.\"\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "assert isinstance(input_ids, torch.Tensor)\n",
    "input_ids = input_ids.to(model.device)\n",
    "assert isinstance(input_ids, torch.Tensor)\n",
    "output = model.generate(input_ids)\n",
    "\n",
    "structured_output = model.engine.parse_structured_output()\n",
    "print(f\"raw output:\\n\\x1b[33m{structured_output}\\x1b[0m\")\n",
    "print(100 * \"-\")\n",
    "print(structured_output.split(answer_delimiters[0])[1].split(answer_delimiters[1])[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
