{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vclIoa6a26VE",
        "outputId": "991e175b-374d-4963-ae54-b5c7c486a6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\u001b[2mResolved \u001b[1m26 packages\u001b[0m \u001b[2min 95ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
            "\u001b[2mAudited \u001b[1m26 packages\u001b[0m \u001b[2min 0.09ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 47ms\u001b[0m\u001b[0m                                           \u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.08ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "#@title Install required packages\n",
        "!uv pip install -U pse # proxy structuring engine\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install accelerate\n",
        "!uv pip install transformers\n",
        "!uv pip install torch\n",
        "!uv pip install numpy\n",
        "!uv pip install bitsandbytes\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install protobuf\n",
        "!uv pip install -U tqdm\n",
        "!uv pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Setup Llama 3.2 1B and create the StructuringEngine\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "\n",
        "from pse.engine.structuring_engine import StructuringEngine\n",
        "from pse.util.torch_mixin import PSETorchMixin\n",
        "\n",
        "\n",
        "class PSE_Torch(PSETorchMixin, LlamaForCausalLM):\n",
        "    pass\n",
        "\n",
        "\n",
        "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = PSE_Torch.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id[0]\n",
        "if model.generation_config:\n",
        "    model.generation_config.top_p = None\n",
        "    model.generation_config.top_k = 8\n",
        "    model.generation_config.do_sample = True\n",
        "    model.generation_config.temperature = 0.9\n",
        "    model.generation_config.pad_token_id = model.config.eos_token_id[0]\n",
        "    model.generation_config.max_new_tokens = 1000\n",
        "\n",
        "model.engine = StructuringEngine(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiylAN6ZB0F8",
        "outputId": "273965a0-2bec-4729-b0f4-386da9249fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"value\": 9.11\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title test simple json object generation\n",
        "import json\n",
        "\n",
        "SIMPLE_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\"value\": {\"type\": \"number\"}},\n",
        "    \"required\": [\"value\"],\n",
        "}\n",
        "model.engine.configure(SIMPLE_JSON_SCHEMA)\n",
        "prompt = (\n",
        "    \"Please generate a json object with the value 9.11, with the following schema:\\n\"\n",
        ")\n",
        "prompt += json.dumps(SIMPLE_JSON_SCHEMA, indent=2)\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "# you can print the prompt + output:\n",
        "#   print(tokenizer.decode(output[0]))\n",
        "# you can also access just the structured output:\n",
        "#   engine.parse_structured_output()\n",
        "structured_output = model.engine.parse_structured_output(output_type=dict)\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"arguments\": {\n",
            "    \"chain_of_thought\": [\n",
            "      \"The process of thinking about one's own thought processes. It involves analyzing and evaluating our internal dialogue to improve our metacognition.\\n\",\n",
            "      \"A tool for reflecting on our thoughts, feelings, and behaviors to gain a deeper understanding of ourselves.\\n\",\n",
            "      \"A mindset that enables us to think critically and intentionally, recognizing the role of our thoughts and emotions in shaping our experiences.\\n\"\n",
            "    ]\n",
            "  },\n",
            "  \"name\": \"metacognition\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# @title Test advanced-json generation\n",
        "ADVANCED_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"const\": \"metacognition\"},\n",
        "        \"arguments\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"chain_of_thought\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"description\": \"A sequence of high-level thoughts, reasoning and internal dialogue.\\nThe chain of thought should be a sequence of thoughts that are related to the question.\\n\",\n",
        "                    \"items\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"minLength\": 20,\n",
        "                        \"maxLength\": 200,\n",
        "                    },\n",
        "                    \"minItems\": 1,  # floor the number of thoughts\n",
        "                    \"maxItems\": 3,  # limit the number of thoughts\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"chain_of_thought\"],\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"name\", \"arguments\"],\n",
        "}\n",
        "model.engine.configure(ADVANCED_JSON_SCHEMA)\n",
        "raw_prompt = (\n",
        "    f\"This is a test of your abilities.\"\n",
        "    f\"Please format your response to follow the following schema:\\n{json.dumps(ADVANCED_JSON_SCHEMA, indent=2)}\\n\"\n",
        ")\n",
        "messages = [{\"role\": \"user\", \"content\": raw_prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "greedy_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "structured_output = model.engine.parse_structured_output(output_type=dict)\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"x_pos\": 100,\n",
            "  \"y_pos\": 100,\n",
            "  \"left_click\": true\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# @title Test pydantic generation\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class CursorPositionModel(BaseModel):\n",
        "    \"\"\"\n",
        "    An object representing the position and click state of a cursor.\n",
        "\n",
        "    Attributes:\n",
        "        x_pos: The horizontal position of the cursor in pixels\n",
        "        y_pos: The vertical position of the cursor in pixels\n",
        "        left_click: Whether the left mouse button is currently pressed. Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    x_pos: int\n",
        "    y_pos: int\n",
        "    left_click: bool = False\n",
        "\n",
        "\n",
        "json_schema: dict = model.engine.configure(\n",
        "    CursorPositionModel, json_delimiters=(\"<cursor>\", \"</cursor>\")\n",
        ")\n",
        "prompt = (\n",
        "    \"Please use the following schema to generate a cursor position:\\n\"\n",
        "    f\"{json.dumps(json_schema, indent=2)}.\\n\"\n",
        "    \"Pretend to move the cursor to x = 100 and y = 100, with the left mouse button clicked.\\n\"\n",
        "    \"Wrap your response in <cursor>CursorPositionModel</cursor>.\"\n",
        ")\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "structured_output = model.engine.parse_structured_output(\n",
        "    output_type=CursorPositionModel\n",
        ")\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output.model_dump(), indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
