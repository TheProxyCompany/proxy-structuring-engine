{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vclIoa6a26VE",
        "outputId": "991e175b-374d-4963-ae54-b5c7c486a6ad"
      },
      "outputs": [],
      "source": [
        "#@title Install required packages\n",
        "!uv pip install -U pse # proxy structuring engine\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install accelerate\n",
        "!uv pip install transformers\n",
        "!uv pip install torch\n",
        "!uv pip install numpy\n",
        "!uv pip install bitsandbytes\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install protobuf\n",
        "!uv pip install -U tqdm\n",
        "!uv pip install ipywidgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Setup Llama 3.2 1B and create the StructuringEngine\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "\n",
        "from pse.engine.structuring_engine import StructuringEngine\n",
        "from pse.util.torch_mixin import PSETorchMixin\n",
        "\n",
        "\n",
        "class PSE_Torch(PSETorchMixin, LlamaForCausalLM):\n",
        "    pass\n",
        "\n",
        "\n",
        "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = PSE_Torch.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id[0]\n",
        "if model.generation_config:\n",
        "    model.generation_config.top_p = None\n",
        "    model.generation_config.top_k = 8\n",
        "    model.generation_config.do_sample = True\n",
        "    model.generation_config.temperature = 0.9\n",
        "    model.generation_config.pad_token_id = model.config.eos_token_id[0]\n",
        "    model.generation_config.max_new_tokens = 1000\n",
        "\n",
        "model.engine = StructuringEngine(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiylAN6ZB0F8",
        "outputId": "273965a0-2bec-4729-b0f4-386da9249fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"value\": 9.11\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title test simple json object generation\n",
        "import json\n",
        "\n",
        "SIMPLE_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\"value\": {\"type\": \"number\"}},\n",
        "    \"required\": [\"value\"],\n",
        "}\n",
        "model.engine.configure(SIMPLE_JSON_SCHEMA)\n",
        "prompt = (\n",
        "    \"Please generate a json object with the value 9.11, with the following schema:\\n\"\n",
        ")\n",
        "prompt += json.dumps(SIMPLE_JSON_SCHEMA, indent=2)\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "# you can print the prompt + output:\n",
        "#   print(tokenizer.decode(output[0]))\n",
        "# you can also access just the structured output:\n",
        "#   engine.parse_structured_output()\n",
        "structured_output = model.engine.parse_structured_output(output_type=dict)\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"chain_of_thought\": [\n",
            "    \"The concept of tool calling AI refers to a process where a tool is able to interact with another tool or a human user through a digital interface, such as a chatbot or a voice assistant.\",\n",
            "    \"This concept has gained significant attention in recent years due to advancements in natural language processing (NLP) and machine learning (ML) technologies.\",\n",
            "    \"One possible way AI can be used as a 'tool' is by providing users with a more human-like conversation experience, allowing users to ask follow-up questions and receive more detailed responses.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# @title Test advanced-json generation\n",
        "ADVANCED_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"description\": \"A tool that structures high-level thoughts, reasoning and internal dialogue.\\n Used for step by step reasoning.\",\n",
        "    \"properties\": {\n",
        "        \"chain_of_thought\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"string\",\n",
        "                \"minLength\": 20, # minimum length of a thought (optional)\n",
        "                \"maxLength\": 200, # maximum length of a thought (optional)\n",
        "            },\n",
        "            \"minItems\": 1,  # floor the number of thoughts (optional)\n",
        "            \"maxItems\": 3,  # limit the number of thoughts (optional)\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"chain_of_thought\"],\n",
        "}\n",
        "model.engine.configure(ADVANCED_JSON_SCHEMA)\n",
        "raw_prompt = (\n",
        "    f\"Please follow the following schema when generating your response:\\n{json.dumps(ADVANCED_JSON_SCHEMA, indent=2)}\\n\"\n",
        "    f\"Generate a chain of thought reflecting on the concept of tool calling ai.\"\n",
        ")\n",
        "messages = [{\"role\": \"user\", \"content\": raw_prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "greedy_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "structured_output = model.engine.parse_structured_output(output_type=dict)\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"x_pos\": 100,\n",
            "  \"y_pos\": 100,\n",
            "  \"left_click\": true\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# @title Test pydantic generation\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class CursorPositionModel(BaseModel):\n",
        "    \"\"\"\n",
        "    An object representing the position and click state of a cursor.\n",
        "\n",
        "    Attributes:\n",
        "        x_pos: The horizontal position of the cursor in pixels\n",
        "        y_pos: The vertical position of the cursor in pixels\n",
        "        left_click: Whether the left mouse button is currently pressed. Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    x_pos: int\n",
        "    y_pos: int\n",
        "    left_click: bool = False\n",
        "\n",
        "\n",
        "json_schema: dict = model.engine.configure(\n",
        "    CursorPositionModel, json_delimiters=(\"<cursor>\", \"</cursor>\")\n",
        ")\n",
        "prompt = (\n",
        "    \"Please use the following schema to generate a cursor position:\\n\"\n",
        "    f\"{json.dumps(json_schema, indent=2)}.\\n\"\n",
        "    \"Pretend to move the cursor to x = 100 and y = 100, with the left mouse button clicked.\\n\"\n",
        "    \"Wrap your response in <cursor>CursorPositionModel</cursor>.\"\n",
        ")\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "structured_output = model.engine.parse_structured_output(\n",
        "    output_type=CursorPositionModel\n",
        ")\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output.model_dump(), indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
