{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vclIoa6a26VE",
        "outputId": "991e175b-374d-4963-ae54-b5c7c486a6ad"
      },
      "outputs": [],
      "source": [
        "#@title Install required packages\n",
        "!uv pip install -U pse # proxy structuring engine\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install accelerate\n",
        "!uv pip install transformers\n",
        "!uv pip install torch\n",
        "!uv pip install numpy\n",
        "!uv pip install bitsandbytes\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install protobuf\n",
        "!uv pip install -U tqdm\n",
        "!uv pip install ipywidgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "\n",
        "from pse.engine.structuring_engine import StructuringEngine\n",
        "from pse.util.torch_mixin import PSETorchMixin\n",
        "\n",
        "# toggle this to logging.DEBUG to see the PSE debug logs!\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class PSE_Torch(PSETorchMixin, LlamaForCausalLM):\n",
        "    pass\n",
        "\n",
        "\n",
        "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = PSE_Torch.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id[0]\n",
        "if model.generation_config:\n",
        "    model.generation_config.top_p = None\n",
        "    model.generation_config.top_k = 8\n",
        "    model.generation_config.do_sample = True\n",
        "    model.generation_config.temperature = 1.0\n",
        "    model.generation_config.min_p = 0.02\n",
        "    model.generation_config.max_new_tokens = 1000\n",
        "    model.generation_config.generation_kwargs = {\"logits_to_keep\": 8}\n",
        "    model.generation_config.pad_token_id = model.config.eos_token_id[0]\n",
        "\n",
        "# create structuring engine normally\n",
        "model.engine = StructuringEngine(tokenizer, multi_token_sampling=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiylAN6ZB0F8",
        "outputId": "273965a0-2bec-4729-b0f4-386da9249fc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"value\": 9.11\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title test simple json object generation\n",
        "import json\n",
        "\n",
        "SIMPLE_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\"value\": {\"type\": \"number\"}},\n",
        "    \"required\": [\"value\"],\n",
        "}\n",
        "model.engine.configure(SIMPLE_JSON_SCHEMA)\n",
        "prompt = (\n",
        "    \"Please generate a json object with the value 9.11, with the following schema:\\n\"\n",
        ")\n",
        "prompt += json.dumps(SIMPLE_JSON_SCHEMA, indent=2)\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "# you can print the prompt + output:\n",
        "#   print(tokenizer.decode(output[0]))\n",
        "# you can also access just the structured output:\n",
        "#   engine.parse_structured_output()\n",
        "structured_output = model.engine.parse_structured_output(output_type=dict)\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"chain_of_thought\": [\n",
            "    \"As I consider the concept of AI, I begin to think about its origins.\",\n",
            "    \"The idea of creating artificial intelligence dates back to the 1950s and 1960s with pioneers like Alan Turing and Marvin Minsky.\",\n",
            "    \"Since then, AI has evolved through various forms and applications, from rule-based systems to machine learning algorithms and now, with the rise of deep learning.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# @title Test advanced-json generation\n",
        "ADVANCED_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"description\": \"A tool that structures high-level thoughts, reasoning and internal dialogue.\\n Used for step by step reasoning.\",\n",
        "    \"properties\": {\n",
        "        \"chain_of_thought\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"string\",\n",
        "                \"minLength\": 20, # minimum length of a thought (optional)\n",
        "                \"maxLength\": 200, # maximum length of a thought (optional)\n",
        "            },\n",
        "            \"minItems\": 1,  # floor the number of thoughts (optional)\n",
        "            \"maxItems\": 3,  # limit the number of thoughts (optional)\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"chain_of_thought\"],\n",
        "}\n",
        "model.engine.configure(ADVANCED_JSON_SCHEMA)\n",
        "raw_prompt = (\n",
        "    f\"Please use this metacognition tool to generate a chain of thought.\\n\"\n",
        "    f\"Follow this schema:\\n{json.dumps(ADVANCED_JSON_SCHEMA, indent=2)}\\n\"\n",
        "    \"Think about what it would mean to be a demonstration, only existing as a demonstration.\\n\"\n",
        ")\n",
        "messages = [{\"role\": \"user\", \"content\": raw_prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "greedy_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "structured_output = model.engine.parse_structured_output(output_type=dict)\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "{\n",
            "  \"x_pos\": 100,\n",
            "  \"y_pos\": 100,\n",
            "  \"left_click\": false\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# @title Test pydantic generation\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class CursorPositionModel(BaseModel):\n",
        "    \"\"\"\n",
        "    An object representing the position and click state of a cursor.\n",
        "\n",
        "    Attributes:\n",
        "        x_pos: The horizontal position of the cursor in pixels\n",
        "        y_pos: The vertical position of the cursor in pixels\n",
        "        left_click: Whether the left mouse button is currently pressed. Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    x_pos: int\n",
        "    y_pos: int\n",
        "    left_click: bool = False\n",
        "\n",
        "\n",
        "json_schema: dict = model.engine.configure(\n",
        "    CursorPositionModel, json_delimiters=(\"<cursor>\", \"</cursor>\")\n",
        ")\n",
        "prompt = (\n",
        "    \"Please use the following schema to generate a cursor position:\\n\"\n",
        "    f\"{json.dumps(json_schema, indent=2)}.\\n\"\n",
        "    \"Pretend to move the cursor to x = 100 and y = 100, with the left mouse button clicked.\\n\"\n",
        "    \"Wrap your response in <cursor>CursorPositionModel</cursor>.\"\n",
        ")\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "structured_output = model.engine.parse_structured_output(\n",
        "    output_type=CursorPositionModel\n",
        ")\n",
        "print(100 * \"-\")\n",
        "print(json.dumps(structured_output.model_dump(), indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
