{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vclIoa6a26VE",
        "outputId": "991e175b-374d-4963-ae54-b5c7c486a6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m26 packages\u001b[0m \u001b[2min 171ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
            "\u001b[2mAudited \u001b[1m26 packages\u001b[0m \u001b[2min 0.13ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 34ms\u001b[0m\u001b[0m                                           \u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.09ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title Install required packages\n",
        "!uv pip install -U pse # proxy structuring engine\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install accelerate\n",
        "!uv pip install transformers\n",
        "!uv pip install torch\n",
        "!uv pip install numpy\n",
        "!uv pip install bitsandbytes\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install protobuf\n",
        "!uv pip install -U tqdm\n",
        "!uv pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Setup Llama 3.2 1B\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "\n",
        "from pse.engine.structuring_engine import StructuringEngine\n",
        "from pse.util.torch_mixin import PSETorchMixin\n",
        "\n",
        "\n",
        "class PSE_Torch(PSETorchMixin, LlamaForCausalLM):\n",
        "    pass\n",
        "\n",
        "\n",
        "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = PSE_Torch.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id[0]\n",
        "if model.generation_config:\n",
        "    model.generation_config.pad_token_id = model.config.eos_token_id[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiylAN6ZB0F8",
        "outputId": "273965a0-2bec-4729-b0f4-386da9249fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 12 Feb 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Please generate a json object with the value 9.11, with the following schema:\n",
            "{\n",
            "  \"type\": \"object\",\n",
            "  \"properties\": {\n",
            "    \"value\": {\n",
            "      \"type\": \"number\"\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"value\"\n",
            "  ]\n",
            "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "{\n",
            "  \"value\": 9.11\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Create engine and test json generation\n",
        "import json\n",
        "\n",
        "SIMPLE_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\"value\": {\"type\": \"number\"}},\n",
        "    \"required\": [\"value\"],\n",
        "}\n",
        "model.engine = StructuringEngine(tokenizer)\n",
        "model.engine.configure(SIMPLE_JSON_SCHEMA)\n",
        "prompt = \"Please generate a json object with the value 9.11, with the following schema:\\n\"\n",
        "prompt += json.dumps(SIMPLE_JSON_SCHEMA, indent=2)\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=1.0,\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * \"-\")\n",
        "print(tokenizer.decode(output[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
