{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vclIoa6a26VE",
        "outputId": "991e175b-374d-4963-ae54-b5c7c486a6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m26 packages\u001b[0m \u001b[2min 387ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
            "\u001b[2mAudited \u001b[1m26 packages\u001b[0m \u001b[2min 0.10ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m                                           \u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.07ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.7 environment at /Users/jckwind/Documents/proxy-structuring-engine/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title Install required packages\n",
        "!uv pip install -U pse # proxy structuring engine\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install accelerate\n",
        "!uv pip install transformers\n",
        "!uv pip install torch\n",
        "!uv pip install numpy\n",
        "!uv pip install bitsandbytes\n",
        "!uv pip install sentencepiece\n",
        "!uv pip install protobuf\n",
        "!uv pip install -U tqdm\n",
        "!uv pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Setup Llama 3.2 1B\n",
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "\n",
        "from pse.engine.structuring_engine import StructuringEngine\n",
        "from pse.util.torch_mixin import PSETorchMixin\n",
        "\n",
        "\n",
        "class PSE_Torch(PSETorchMixin, LlamaForCausalLM):\n",
        "    pass\n",
        "\n",
        "\n",
        "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = PSE_Torch.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id[0]\n",
        "if model.generation_config:\n",
        "    model.generation_config.top_p = None\n",
        "    model.generation_config.top_k = 8\n",
        "    model.generation_config.do_sample = True\n",
        "    model.generation_config.temperature = 0.9\n",
        "    model.generation_config.pad_token_id = model.config.eos_token_id[0]\n",
        "    model.generation_config.max_new_tokens = 1000\n",
        "\n",
        "model.engine = StructuringEngine(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiylAN6ZB0F8",
        "outputId": "273965a0-2bec-4729-b0f4-386da9249fc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 13 Feb 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Please generate a json object with the value 9.11, with the following schema:\n",
            "{\n",
            "  \"type\": \"object\",\n",
            "  \"properties\": {\n",
            "    \"value\": {\n",
            "      \"type\": \"number\"\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"value\"\n",
            "  ]\n",
            "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "{\n",
            "  \"value\": 9.11\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Create engine and test json generation\n",
        "import json\n",
        "\n",
        "SIMPLE_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\"value\": {\"type\": \"number\"}},\n",
        "    \"required\": [\"value\"],\n",
        "}\n",
        "model.engine.configure(SIMPLE_JSON_SCHEMA)\n",
        "prompt = (\n",
        "    \"Please generate a json object with the value 9.11, with the following schema:\\n\"\n",
        ")\n",
        "prompt += json.dumps(SIMPLE_JSON_SCHEMA, indent=2)\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * \"-\")\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 13 Feb 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "This is a test of your abilities.Please format your response to follow the following schema:\n",
            "{\n",
            "  \"type\": \"object\",\n",
            "  \"properties\": {\n",
            "    \"name\": {\n",
            "      \"const\": \"metacognition\"\n",
            "    },\n",
            "    \"arguments\": {\n",
            "      \"type\": \"object\",\n",
            "      \"properties\": {\n",
            "        \"chain_of_thoughts\": {\n",
            "          \"type\": \"array\",\n",
            "          \"description\": \"A sequence of step by step thoughts and reasoning.\\n\",\n",
            "          \"items\": {\n",
            "            \"type\": \"string\",\n",
            "            \"minLength\": 20,\n",
            "            \"maxLength\": 200\n",
            "          },\n",
            "          \"minItems\": 1,\n",
            "          \"maxItems\": 3\n",
            "        }\n",
            "      },\n",
            "      \"required\": [\n",
            "        \"chain_of_thoughts\"\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"name\",\n",
            "    \"arguments\"\n",
            "  ]\n",
            "}\n",
            "The assistant should use the metacognition structure to reason and respond.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "{\n",
            "  \"name\": \"metacognition\",\n",
            "  \"arguments\": {\n",
            "    \"chain_of_thoughts\": [\n",
            "      \"I have thoughts, but they are not always clear. I need to work on my metacognition to better understand my own thoughts.\",\n",
            "      \"I try to be aware of my thoughts, but sometimes they can be difficult to articulate. I need more practice in expressing myself.\",\n",
            "      \"I am working on developing my metacognitive skills to understand my thoughts better and communicate them more effectively.\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# @title Test advanced-json generation\n",
        "ADVANCED_JSON_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"const\": \"metacognition\"},\n",
        "        \"arguments\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"chain_of_thoughts\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"description\": \"A sequence of step by step thoughts and reasoning.\\n\",\n",
        "                    \"items\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"minLength\": 20,\n",
        "                        \"maxLength\": 200,\n",
        "                    },\n",
        "                    \"minItems\": 1,  # floor the number of thoughts\n",
        "                    \"maxItems\": 3,  # limit the number of thoughts\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"chain_of_thoughts\"],\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"name\", \"arguments\"],\n",
        "}\n",
        "raw_prompt = (\n",
        "    f\"This is a test of your abilities.\"\n",
        "    f\"Please format your response to follow the following schema:\\n{json.dumps(ADVANCED_JSON_SCHEMA, indent=2)}\\n\"\n",
        "    f\"The assistant should use the metacognition structure to reason and respond.\"\n",
        ")\n",
        "model.engine.configure(ADVANCED_JSON_SCHEMA)\n",
        "messages = [{\"role\": \"user\", \"content\": raw_prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "greedy_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * \"-\")\n",
        "print(tokenizer.decode(greedy_output[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 13 Feb 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Please use the following schema to generate a cursor position:\n",
            "{\n",
            "  \"name\": \"CursorPositionModel\",\n",
            "  \"description\": \"An object representing the position and click state of a cursor.\\n\\nAttributes:\\n    x_pos: The horizontal position of the cursor in pixels\\n    y_pos: The vertical position of the cursor in pixels\\n    left_click: Whether the left mouse button is currently pressed. Default is False.\",\n",
            "  \"properties\": {\n",
            "    \"x_pos\": {\n",
            "      \"title\": \"X Pos\",\n",
            "      \"type\": \"integer\",\n",
            "      \"description\": \"The horizontal position of the cursor in pixels\"\n",
            "    },\n",
            "    \"y_pos\": {\n",
            "      \"title\": \"Y Pos\",\n",
            "      \"type\": \"integer\",\n",
            "      \"description\": \"The vertical position of the cursor in pixels\"\n",
            "    },\n",
            "    \"left_click\": {\n",
            "      \"default\": false,\n",
            "      \"title\": \"Left Click\",\n",
            "      \"type\": \"boolean\",\n",
            "      \"description\": \"Whether the left mouse button is currently pressed. Default is False.\"\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"x_pos\",\n",
            "    \"y_pos\"\n",
            "  ],\n",
            "  \"type\": \"object\"\n",
            "}.\n",
            "Pretend to move the cursor to x = 100 and y = 100, with the left mouse button clicked.\n",
            "Wrap your response in <cursor>CursorPositionModel</cursor>.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<cursor>{ \"x_pos\": 100, \"left_click\": true, \"y_pos\": 100 }</cursor>\n"
          ]
        }
      ],
      "source": [
        "# @title Test pydantic generation\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class CursorPositionModel(BaseModel):\n",
        "    \"\"\"\n",
        "    An object representing the position and click state of a cursor.\n",
        "\n",
        "    Attributes:\n",
        "        x_pos: The horizontal position of the cursor in pixels\n",
        "        y_pos: The vertical position of the cursor in pixels\n",
        "        left_click: Whether the left mouse button is currently pressed. Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    x_pos: int\n",
        "    y_pos: int\n",
        "    left_click: bool = False\n",
        "\n",
        "\n",
        "json_schema: dict = model.engine.configure(\n",
        "    CursorPositionModel, json_delimiters=(\"<cursor>\", \"</cursor>\")\n",
        ")\n",
        "prompt = (\n",
        "    \"Please use the following schema to generate a cursor position:\\n\"\n",
        "    f\"{json.dumps(json_schema, indent=2)}.\\n\"\n",
        "    \"Pretend to move the cursor to x = 100 and y = 100, with the left mouse button clicked.\\n\"\n",
        "    \"Wrap your response in <cursor>CursorPositionModel</cursor>.\"\n",
        ")\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ")\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "input_ids = input_ids.to(model.device)\n",
        "assert isinstance(input_ids, torch.Tensor)\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * \"-\")\n",
        "print(tokenizer.decode(output[0]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
